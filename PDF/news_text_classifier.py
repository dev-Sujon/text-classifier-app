# -*- coding: utf-8 -*-
"""news_text_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-rAyNtIcnWCLVPWVQxQDt0xhQSLj2VZx

# News text classification web app

About Dataset


---


This is text document classification dataset which contains 2225 text data and five categories of documents. Five categories are politics, sport, tech, entertainment and business. We can use this dataset for documents classification and document clustering.

Dataset contains two features text and label.

No. of Rows : 2225

No. of Columns : 2

Text:It contains different categories of text data

**Label: **It contains labels for five different categories : 0,1,2,3,4

Politics = 0

Sport = 1

Technology = 2

Entertainment =3

Business = 4

## 1. Data Preparation and Exploration

* Collect and Load Data

 * Understend the newspaper dataset with varied sentiment labels (positive, negative, neutral).

 * Examples: Scrape or obtain a labeled dataset like News Aggregator, or create labels if starting with raw text.

* Data Exploration and Cleaning

 * Check for data imbalances in sentiment categories and handle them (e.g., through resampling).
 * Clean text data (remove special characters, irrelevant symbols, whitespace, etc.).

* Conduct exploratory data analysis (EDA) to understand data distribution, length of articles, sentiment label distribution.
* Data Splitting

 * Split data into training, validation, and test sets. Aim for a typical split of 70% train, 15% validation, and 15% test.
"""

# collect the data from my github repository
import pandas as pd

file_url = "https://github.com/dev-Sujon/Branching_Tools/raw/refs/heads/master/df_file.csv"

try:
    df = pd.read_csv(file_url)
    print(df.head())  # Display the first few rows of the DataFrame
except FileNotFoundError:
    print(f"Error: File not found at {file_url}. Please check the URL.")
except Exception as e:
    print(f"An error occurred: {e}")

print("shape of the dataset is :", df.shape)

df.head()

df.describe()

# check if there any null value in the dataset
df.isnull().sum()

"""No fissing value

Summary

There are no "Null" values, so no need to replace anything.

Dataset had 98 duplicated rows that have been dropped.

Distribution of labels ranges between 17% - 24% which are acceptable.
"""

import pandas as pd
# Define the label encoding mapping
label_mapping = {
    0:'Politics',
    2:'Sport',
    3:'Technology',
    4:'Entertainment',
    5:'Business'
}

# Apply the label encoding
df['Label_encoded'] = df['Label'].map(label_mapping)

# Display the updated DataFrame
print(df)

import matplotlib.pyplot as plt
plt.style.use('seaborn-v0_8-pastel')

fig, axs = plt.subplots(1, 2, figsize=(20, 6), gridspec_kw={'width_ratios': [1, 2]})

# Pie Chart
ax1 = axs[0]
subject = df['Label'].value_counts().reset_index(name='Count')
ax1.pie(subject['Count'], labels=subject['Label'], autopct='%1.1f%%', radius=1.2, startangle=20)
ax1.set_title(f'Share of Categories')

# Bar Chart
ax2 = axs[1]
subject.plot(kind='bar', x='Label', ax=ax2)
ax2.set_title('Category counts')
ax2.set_xlabel('Categories')
ax2.set_ylabel('Count')
ax2.set_xticklabels(ax2.get_xticklabels(), rotation=0)
ax2.grid(True)

plt.show()

from wordcloud import WordCloud, STOPWORDS

all_text = ' '.join(df['Text'])
wc = WordCloud(width=800,
               height=400,
               background_color='white',
               max_words=100).generate(all_text)
plt.figure(figsize=(10,5))
plt.imshow(wc)
plt.axis('off')
plt.show()

"""## 2. Model Selection and Preparation

* Choose a Pretrained Model

 * Start with a base sentiment analysis model from Hugging Face (e.g., bert-base-uncased, distilbert-base-uncased, or a model like BERT fine-tuned on sentiment).
 * Load the model using the Transformers library.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
import re

df['Text'][100]

df.tail()

import os
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import string
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Ensure you have the necessary NLTK resources
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text_series):
    # Initialize lemmatizer
    lemmatizer = WordNetLemmatizer()

    cleaned_texts = []

    for text in text_series:
        if isinstance(text, str):  # Check if the item is a string
            # Convert text to lowercase
            text = text.lower()

            # Remove punctuation
            text = text.translate(str.maketrans("", "", string.punctuation))

            # Remove numbers
            text = re.sub(r'\d+', '', text)

            # Tokenize text
            tokens = word_tokenize(text)

            # Remove stop words
            stop_words = set(stopwords.words("english"))
            tokens = [word for word in tokens if word not in stop_words]

            # Lemmatize tokens
            tokens = [lemmatizer.lemmatize(word) for word in tokens]

            # Join tokens back into a string
            cleaned_text = " ".join(tokens)
        else:
            cleaned_text = ''  # If not a string, you can handle it as you wish

        cleaned_texts.append(cleaned_text)

    return cleaned_texts

# Assuming you already have your DataFrame named `df`
# Preprocess the 'Text' column and add cleaned text to a new column
df['cleaned_text'] = preprocess_text(df["Text"])

# Display original and cleaned text
print("Original text:")
print(df["Text"])

print("\nCleaned text:")
print(df["cleaned_text"])

df.head(5)

"""* Tokenization and Data Preprocessing

 * Tokenize your dataset using the tokenizer associated with your chosen model.
 * Pad and truncate sequences to a fixed length to handle batch processing.
 * Convert data into Dataset objects compatible with Hugging Face's Datasets library for efficient handling.
"""

df.shape

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from transformers import AutoTokenizer

# Ensure you have NLTK data downloaded
nltk.download('punkt')

# Assuming df is your existing DataFrame and 'cleaned_text' is the column of interest

# Initialize the Hugging Face tokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Lists to store tokenized sentences and words
all_tokenized_sentences = []
all_tokenized_words = []
all_huggingface_tokens = []
all_input_ids = []

# Iterate over each row in the cleaned_text column
for text in df['cleaned_text']:
    # NLTK Tokenization
    # Tokenizing sentences
    sentences = sent_tokenize(text)
    all_tokenized_sentences.append(sentences)
    print("Tokenized Sentences:")
    print(sentences)

    # Tokenizing words
    words = word_tokenize(text)
    all_tokenized_words.append(words)


    # Hugging Face Tokenization
    # Tokenizing text
    tokens = tokenizer.tokenize(text)
    all_huggingface_tokens.append(tokens)


    # Converting tokens to input IDs
    input_ids = tokenizer.encode(text, add_special_tokens=True)
    all_input_ids.append(input_ids)


# Optionally, you can add these lists back to your DataFrame
df['tokenized_sentences'] = all_tokenized_sentences
df['tokenized_words'] = all_tokenized_words
df['huggingface_tokens'] = all_huggingface_tokens
df['input_ids'] = all_input_ids

df.tail(1)

df.head(2)

# @title Label

from matplotlib import pyplot as plt
df['Label'].plot(kind='hist', bins=20, title='Label')
plt.gca().spines[['top', 'right',]].set_visible(False)

df.shape

"""## 3. Fine-Tuning the Model on Newspaper Data

"""

!pip install transformers torch

"""* Setup Training Configuration

 * Define hyperparameters: batch size, learning rate, number of epochs, and early stopping criteria.
 * Use Hugging Faceâ€™s Trainer API for streamlined model fine-tuning on your newspaper dataset.
 * Fine-tune with gradual unfreezing: initially freeze some model layers to retain pretrained language knowledge and gradually unfreeze layers as training progresses.

"""

!pip install datasets

import torch
from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizerFast
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict

# Load the tokenizer and model
model_name = "bert-base-uncased"
tokenizer = BertTokenizerFast.from_pretrained(model_name)

model = BertForSequenceClassification.from_pretrained(model_name, num_labels=5)

df

# train test split

train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)
train_dataset = Dataset.from_pandas(train_data[['input_ids', 'Label_encoded']])
test_dataset = Dataset.from_pandas(train_data[['input_ids', 'Label_encoded']])

df.head(2)

# Define the training arguments with matching save and evaluation strategies
training_args = TrainingArguments(
    output_dir='./results',           # output directory
    evaluation_strategy="epoch",      # evaluate each epoch
    save_strategy="epoch",            # save model checkpoint each epoch
    learning_rate=2e-5,
    per_device_train_batch_size=8,    # batch size for training
    per_device_eval_batch_size=8,     # batch size for evaluation
    num_train_epochs=3,               # number of training epochs
    weight_decay=0.01,                # strength of weight decay
    logging_dir='./logs',             # directory for storing logs
    load_best_model_at_end=True
)

"""
* Train and Evaluate

 * Start training and monitor model performance (accuracy, F1-score) on the validation set.
 * Use metrics to track progress and ensure that your model generalizes well to unseen data."""

!pip install peft

df

import os
import torch
import pandas as pd
from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizerFast
from peft import get_peft_model, LoraConfig, TaskType
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict

# Disable Weights and Biases logging
os.environ["WANDB_MODE"] = "disabled"

# Convert categorical labels to integers
label_mapping = {label: idx for idx, label in enumerate(df['Label_encoded'].unique())}
df['labels'] = df['Label_encoded'].map(label_mapping)

# Load tokenizer and base model
model_name = "bert-base-uncased"
tokenizer = BertTokenizerFast.from_pretrained(model_name)
base_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_mapping))

# Configure LoRA for the model
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=16,
    lora_dropout=0.1
)

# Load the base model and apply LoRA
model = get_peft_model(base_model, lora_config)

# Split the dataset
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)

# Tokenize the data with padding and truncation
def tokenize_function(example):
    return tokenizer(example['cleaned_text'], padding="max_length", truncation=True)

# Create Hugging Face datasets and tokenize
train_dataset = Dataset.from_pandas(train_data[['cleaned_text', 'labels']])
test_dataset = Dataset.from_pandas(test_data[['cleaned_text', 'labels']])
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Set format to tensors for compatibility with PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

# Prepare DatasetDict for Trainer
dataset = DatasetDict({
    'train': train_dataset,
    'test': test_dataset
})

train_dataset

from transformers import Trainer, TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    save_strategy="epoch",
    load_best_model_at_end=True
)

# Select only the first 500 and 100 train test samples from train and test datasets
train_subset = dataset['train'].select(range(500))
eval_subset = dataset['test'].select(range(100))

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_subset,
    eval_dataset=eval_subset,
    tokenizer=tokenizer
)

# Fine-tune the model
trainer.train()

# Save the fine-tuned model and tokenizer
model.save_pretrained("finetuned-bert-lora-news-sentiment")
tokenizer.save_pretrained("finetuned-bert-lora-tokenizer")

# Evaluate the model
eval_results = trainer.evaluate()
print(f"Evaluation Results: {eval_results}\m")

df['Text'][0]

df["Text"][0]

"""## 4. Integrate LangChain for Enhanced Data Processing and Interaction

* Use LangChain for Data Preprocessing Pipelines

 * Leverage LangChain's chain functionalities to preprocess and augment text data automatically.
 * Example: Build a pipeline that identifies keywords, topics, or entities in news articles for a richer sentiment analysis.
* Incorporate Retrieval-Augmented Generation (Optional)

 * If exploring RAG, integrate an indexing solution (e.g., FAISS or ElasticSearch) to store news articles and retrieve relevant context during inference.

 * Implement a LangChain retrieval chain to provide background information on news topics, enhancing interpretability for each sentiment prediction.

## 5. Optimize Model Performance

* Experiment with Advanced Techniques
 * Try model distillation (if using a large model) to reduce model size and inference time.
 * Experiment with knowledge distillation, where a smaller student model learns from your fine-tuned model (teacher) to produce a faster version without losing accuracy.
* Hyperparameter Tuning
 * Perform grid search or use tools like Optuna or Ray Tune to optimize hyperparameters for better performance.
 * Optimize based on validation metrics to avoid overfitting on the  training set.

## 6. Model Evaluation and Testing

* Evaluate on Test Set

 * Measure final performance on the test set to evaluate real-world performance.
 * Calculate metrics such as accuracy, F1-score, precision, recall, and confusion matrix for a comprehensive view of model performance across sentiment classes.

* Error Analysis

 * Analyze incorrect predictions to identify model weaknesses or biases.
 * Consider using LangChainâ€™s chain functionalities for interpretability, where it generates an explanation of why a particular sentiment was predicted.
"""



# prompt: evaluate the model

# Evaluate the model
eval_results = trainer.evaluate()
print(f"Evaluation Results: {eval_results}\n")

# save model in my google drive

from google.colab import drive
drive.mount('/content/drive')

# Save the model to your Google Drive
!cp -r finetuned-bert-lora-news-sentiment /content/drive/MyDrive/
!cp -r finetuned-bert-lora-tokenizer /content/drive/MyDrive/